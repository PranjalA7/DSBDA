{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DS_BDA_LAB_07(TE-A-07).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Text Analytics**\n","1. Extract Sample document and apply following document preprocessing methods:\n","Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n","2. Create representation of document by calculating Term Frequency and Inverse Document\n","Frequency."],"metadata":{"id":"b9KDpLzRo96h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDJSYqTi37Dk"},"outputs":[],"source":["import nltk"]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ncCkwlH4exu","outputId":"40317bbc-605a-40bb-c62f-9545c6413c4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from nltk import word_tokenize, sent_tokenize\n","sent = \"I will walk 500 miles and I would walk 500 more, just to be the man who walks a thousand miles to fall down at your door!\"\n","print(word_tokenize(sent))\n","print(sent_tokenize(sent))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6THhz68Y4I3A","outputId":"9d6bf7f1-85e3-4822-efb7-6a8eba099420"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', ',', 'just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n","['I will walk 500 miles and I would walk 500 more, just to be the man who walks a thousand miles to fall down at your door!']\n"]}]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fne0PV3RF8KM","outputId":"015b5315-ea49-45a1-a93f-fa08b37e9723"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from nltk.corpus import stopwords                                        \n","stop_words = stopwords.words('english')                                  \n","token = word_tokenize(sent)\n","cleaned_token = []\n","for word in token:\n","    if word not in stop_words:\n","        cleaned_token.append(word)\n","print(\"This is the unclean version:\", token)\n","print(\"This is the cleaned version:\", cleaned_token)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kd7tjvLGFlxH","outputId":"bd303949-733d-4bc9-fb7f-150faec79b7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the unclean version: ['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', ',', 'just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n","This is the cleaned version: ['I', 'walk', '500', 'miles', 'I', 'would', 'walk', '500', ',', 'man', 'walks', 'thousand', 'miles', 'fall', 'door', '!']\n"]}]},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n","snowball_stemmer = SnowballStemmer('english')\n","text=\"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n","word_tokens = nltk.word_tokenize(text)\n","stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n","print (stemmed_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5g5dsPLgNBmi","outputId":"92358e35-2f4d-4313-a69b-d4152705742b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['this', 'is', 'a', 'demo', 'text', 'for', 'nlp', 'use', 'nltk', '.', 'full', 'form', 'of', 'nltk', 'is', 'natur', 'languag', 'toolkit']\n"]}]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wphbjzlFRfTO","outputId":"8e600f48-c548-4201-f041-2db857213409"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","stopword = stopwords.words('english')\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"the dogs are barking outside. Are the cats in the garden?\"\n","word_tokens = nltk.word_tokenize(text)\n","lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n","print (lemmatized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjVK2Bn5RFeK","outputId":"197e58f7-42da-4945-c4d3-ab8eb0019020"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'dog', 'are', 'barking', 'outside', '.', 'Are', 'the', 'cat', 'in', 'the', 'garden', '?']\n"]}]},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGzdHYcoTT8j","outputId":"79a34148-1d5e-4d2c-9e35-70b1669fe55b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["text = \"the dogs are barking outside.\"\n","word = nltk.word_tokenize(text)\n","pos_tag = nltk.pos_tag(word)\n","print (pos_tag)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfXtsOCcTCkT","outputId":"39294503-c107-4bdd-81d9-759c78d9698d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('the', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('outside', 'IN'), ('.', '.')]\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"xLugU9w4m_5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","d0 = 'demo of dsbda'\n","d1 = 'dsbda lab'\n","d2 = 'lab assignment'\n","\n","series = [d0, d1, d2]"],"metadata":{"id":"kIBHZgkAnSeL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create object\n","tfidf = TfidfVectorizer()\n","\n","# get tf-df values\n","result = tfidf.fit_transform(series)"],"metadata":{"id":"GOYKEwqHnoNZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get indexing\n","print('\\nWord indexes:')\n","print(tfidf.vocabulary_)\n","\n","# display tf-idf values\n","print('\\ntf-idf value:')\n","print(result)\n","\n","# in matrix form\n","print('\\ntf-idf values in matrix form:')\n","print(result.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_FRWafbn2-D","outputId":"0c5564be-c1c1-41e4-b079-318d0a7f0aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Word indexes:\n","{'demo': 1, 'of': 4, 'dsbda': 2, 'lab': 3, 'assignment': 0}\n","\n","tf-idf value:\n","  (0, 2)\t0.4736296010332684\n","  (0, 4)\t0.6227660078332259\n","  (0, 1)\t0.6227660078332259\n","  (1, 3)\t0.7071067811865476\n","  (1, 2)\t0.7071067811865476\n","  (2, 0)\t0.7959605415681652\n","  (2, 3)\t0.6053485081062916\n","\n","tf-idf values in matrix form:\n","[[0.         0.62276601 0.4736296  0.         0.62276601]\n"," [0.         0.         0.70710678 0.70710678 0.        ]\n"," [0.79596054 0.         0.         0.60534851 0.        ]]\n"]}]}]}