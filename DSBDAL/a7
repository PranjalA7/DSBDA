import nltk
nltk.download('punkt')

from nltk import word_tokenize, sent_tokenize
sent = "I will walk 500 miles and I would walk 500 more, just to be the man who walks a thousand miles to fall down at your door!"
print(word_tokenize(sent))
print(sent_tokenize(sent))

nltk.download('stopwords')

from nltk.corpus import stopwords                                        
stop_words = stopwords.words('english')                                  
token = word_tokenize(sent)
cleaned_token = []
for word in token:
    if word not in stop_words:
        cleaned_token.append(word)
print("This is the unclean version:", token)
print("This is the cleaned version:", cleaned_token)

from nltk.stem import SnowballStemmer
snowball_stemmer = SnowballStemmer('english')
text="This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit"
word_tokens = nltk.word_tokenize(text)
stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]
print (stemmed_word)

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
stopword = stopwords.words('english')
wordnet_lemmatizer = WordNetLemmatizer()
text = "the dogs are barking outside. Are the cats in the garden?"
word_tokens = nltk.word_tokenize(text)
lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]
print (lemmatized_word)

nltk.download('averaged_perceptron_tagger')

text = "the dogs are barking outside."
word = nltk.word_tokenize(text)
pos_tag = nltk.pos_tag(word)
print (pos_tag)

from sklearn.feature_extraction.text import TfidfVectorizer

d0 = 'demo of dsbda'
d1 = 'dsbda lab'
d2 = 'lab assignment'

series = [d0, d1, d2]
# create object
tfidf = TfidfVectorizer()

# get tf-df values
result = tfidf.fit_transform(series)
# get indexing
print('\nWord indexes:')
print(tfidf.vocabulary_)

# display tf-idf values
print('\ntf-idf value:')
print(result)

# in matrix form
print('\ntf-idf values in matrix form:')
print(result.toarray())

